---
title: "Homework 5"
author: Nancy Fang (nf2466)
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1
Read in the data

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv")%>%
  mutate(
    city_state = str_c(city, state, sep="_"),
    resolved = case_when(disposition == "Closed by arrest" ~"solved",
                         disposition == "Closed without arrest" ~"unsolved",
                         disposition == "Open/No arrest" ~"unsolved"))%>%
  select(city_state, resolved)
  
```

The raw data is a CSV file that contains several variables, including the report date, first and last name of the victims, race, age, sex, city, state, location (latitude and longitude) and disposition of the case. The original dataset contains `r nrow(homicide_df)` rows. 

Aggregate data:

```{r}
homicide_sum = 
  homicide_df %>%
    group_by(city_state)%>%
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  )%>%
  filter(city_state != "Tulsa_AL")
```
We know that there are only 50 cities included in the data set, yet the original dataset had 51 cities. When we arranged the `city_state` variable by total homicides, we saw that Tulsa_AL only had one total homicide, which likely was an input error.

Single prop test for Baltimore, MD:

```{r}
  baltimore_prop = 
  prop.test(homicide_sum %>% filter(city_state == "Baltimore_MD") %>% pull(hom_unsolved),
            homicide_sum %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)
  ) %>% 
  broom::tidy() %>%
  select(estimate, conf.low, conf.high)

```
Prop.test for each of the cities in your dataset:

```{r}
results_df = 
  homicide_sum %>%
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  )%>%
  select(-prop_tests)%>%
  unnest(tidy_tests)%>%
  select(city_state,estimate, conf.low, conf.high)

results_df %>%
  knitr::kable()

```

Plot that shows the estimates and CIs for each city:

```{r}
unsolved_plot = 
  results_df %>%
  mutate(city_state = fct_reorder(city_state,estimate)) %>%
  ggplot(aes(x = city_state, y = estimate)) +
    geom_point() +
    geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

unsolved_plot
```

## Problem 2

Create a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time:

Read in data:

```{r}
path_df = 
  tibble(
  path = list.files("data/problem_2/data")) %>%
  mutate(
    path = str_c("data/problem_2/data", path, sep = "/"),
    data = map(.x = path, ~read_csv(.x))
  )%>%
  unnest(data)%>%
  mutate(
    path = str_replace(path,"data/problem_2/data/",""),
    path = str_remove(path, ".csv")
  )%>%
  separate(path, c("arm", "ID"))%>%
  pivot_longer(cols = starts_with("week"),
               names_to = "week",
               names_prefix = "week_",
               values_to = "obs")
spaghetti = 
path_df %>%
  group_by(arm, ID)%>%
  ggplot(aes( x = week, y = obs, group = ID)) +
  geom_line(aes(color=ID))+
  facet_grid(.~arm)

spaghetti
```


From the sphaghetti plot, we can see that compared to the control group, the subjects in the experimental group had an upward trend in value as time went on. The control group values stayed around the same value when compared to the experimental arm.

## Problem 3

Create a function following instructions

```{r}
sim_mean_sd = 
  function(samp_size = 30, mu, sigma = 5) #set n=30 and sigma=5 

  {
  
  sim_data = tibble(
    x = rnorm(samp_size, mean = mu, sd = sigma),
  )%>%                                                  #set model X~Normal[μ,σ]
    t.test(mu = 0, conf.level = 0.95) %>% 
    broom::tidy()%>%
    select(
      estimate, p.value
    )
}

sim_rerun =
  rerun(5000,sim_mean_sd(mu = 0))%>%                  #Generate 5000 datasets from the model when mu = 0
  bind_rows()
  

sim_results =                                         #iteration of function with different mu values
  tibble(mu = c(0, 1, 2, 3, 4, 5, 6)) %>% 
  mutate(
    output_lists = map(.x = mu, ~rerun(5000, sim_mean_sd(mu = .x))),
    estimate_dfs = map(output_lists, bind_rows)) %>% 
  select(-output_lists) %>% 
  unnest(estimate_dfs)
```

Plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis. We see from our plot that as our true mean (μ) moves further away from the null hypothesis (μ=0), the power (proportion of values that reject null over the total number of values) gets closer to 1.


```{r}
reject_sum =
  sim_results %>%
  mutate(
    reject = case_when(p.value < 0.05 ~ "reject",
                     p.value >= 0.05 ~ "do_not_reject")
  )

null_sum =
  reject_sum%>%
  group_by(mu)%>%
  summarize(
    null_total = n(),
    null_reject = sum(reject == "reject")
  )

null_prop = 
  null_sum %>%
  mutate(
    prop_tests = map2(.x = null_reject, .y = null_total, ~prop.test(x=.x, n=.y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  )%>%
  select(-prop_tests)%>%
  unnest(tidy_tests)%>%
  select(mu,estimate, conf.low, conf.high)

null_plot = 
  null_prop %>%
  ggplot(aes(x = mu, y = estimate)) +
    geom_point() +
    geom_errorbar(aes(ymin = conf.low, ymax = conf.high))+
    scale_x_discrete(limit=null_prop$mu)

null_plot
```

Plot showing the average estimate of μ on the y axis and the true value of μ on the x axis. 
Overlaid a second plot the average estimate of $\hat{μ}$ only in samples for which the null was rejected on the y axis and the true value of μ on the x axis. 

The sample average of $\hat{μ}$ across tests for which the null is rejected approximately equal to the true value of μ as the true value of mu increases.As the true value of each sample's μ gets further away from the null, the more likely $\hat{μ}$ will reject the null hypothesis, so the average of mu_estimates in total and that reject the null will be more similar. We see that there is some difference between the mean estimates between the total mean and the mean of the estimates that reject the null hypothesis when μ = 1 and μ = 2 since the proportion of estimates that reject the null are lower (the effect size from the null is smaller).  


```{r}
mu_plot =
  reject_sum %>%
  group_by(mu)%>%
  summarize(
    mean = mean(estimate))%>%
  mutate(
    name = "mu_plot"
  )


mu_reject_plot =
  reject_sum %>%
  filter(p.value<0.05) %>%
  group_by(mu)%>%
  summarize(
    mean = mean(estimate))%>%
  mutate(
    name = "mu_reject_plot"
  )
  
combined_plot = 
  rbind(mu_plot, mu_reject_plot)%>%
  ggplot () +
  geom_point(aes(x = mu, y = mean, color = name),size = 2, alpha = 0.4) +
  geom_line(aes(x = mu, y = mean, color = name), size = 2, alpha = 0.4) +
  xlab('True value of mu')+
  ylab('Mean estimates of mu')

combined_plot
```

